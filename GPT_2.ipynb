{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import re\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from transformers import GPT2Config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get csv file\n",
    "data_path = \"data/raw/data/beetle.csv\"\n",
    "df = pd.read_csv(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove extra white spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Preprocess student and reference answers in the dataset\n",
    "df['student_answer'] = df['student_answer'].apply(preprocess_text)\n",
    "df['reference_answer'] = df['reference_answer'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence_pair(student_answer, reference_answer, max_length=512):\n",
    "    # Set the padding token to the EOS token if not defined\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    tokens = tokenizer(student_answer, reference_answer, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASAGDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        student_answer = row[\"student_answer\"]\n",
    "        reference_answer = row[\"reference_answer\"]\n",
    "        assigned_points = row[\"assigned_points\"]\n",
    "        max_points = row[\"max_points\"]\n",
    "        percentage_of_correctness = row[\"assigned_points\"] / row[\"max_points\"]\n",
    "\n",
    "        encoded = encode_sentence_pair(student_answer, reference_answer)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoded[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(),\n",
    "            \"assigned_points\": torch.tensor(assigned_points, dtype=torch.float32),\n",
    "            \"max_points\": torch.tensor(max_points, dtype=torch.float32),\n",
    "            \"percentage_of_correctness\": torch.tensor(percentage_of_correctness, dtype=torch.float32),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Set the padding token to the EOS token if not defined\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Pad input_ids and attention_mask tensors\n",
    "    input_ids = pad_sequence([item[\"input_ids\"] for item in batch], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = pad_sequence([item[\"attention_mask\"] for item in batch], batch_first=True, padding_value=0)\n",
    "\n",
    "    percentage_of_correctness = torch.tensor([item[\"percentage_of_correctness\"] for item in batch], dtype=torch.float32)\n",
    "    assigned_points = torch.tensor([item[\"assigned_points\"] for item in batch], dtype=torch.float32)\n",
    "    max_points = torch.tensor([item[\"max_points\"] for item in batch], dtype=torch.float32)\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"percentage_of_correctness\": percentage_of_correctness,\n",
    "        \"assigned_points\": assigned_points,\n",
    "        \"max_points\": max_points,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = ASAGDataset(train_df)\n",
    "val_dataset = ASAGDataset(val_df)\n",
    "\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olaf/opt/miniconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Train loss: 0.4552588740144251\n",
      "Epoch 2/3 - Train loss: 0.2015805348663892\n",
      "Epoch 3/3 - Train loss: 0.17757784188063844\n",
      "Mean Squared Error: 0.16920366439962303\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\" if getattr(torch,'has_mps',False) \\\n",
    "    else \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "gpt2_config = GPT2Config.from_pretrained(\"gpt2\", num_labels=1)\n",
    "gpt2_config.pad_token_id = tokenizer.pad_token_id\n",
    "model = GPT2ForSequenceClassification(gpt2_config)\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 3\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        percentage_of_correctness = batch[\"percentage_of_correctness\"].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=percentage_of_correctness.unsqueeze(1))\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Train loss: {train_loss / len(train_dataloader)}\")\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()\n",
    "predictions = []\n",
    "ground_truth = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        percentage_of_correctness = batch[\"percentage_of_correctness\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits.squeeze().detach().cpu()\n",
    "\n",
    "        predictions.extend(logits.tolist())\n",
    "        ground_truth.extend(percentage_of_correctness.cpu().tolist())\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(ground_truth, predictions)\n",
    "print(f\"Mean Squared Error: {mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>question</th>\n",
       "      <th>question_id</th>\n",
       "      <th>student_answer</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>assigned_points</th>\n",
       "      <th>max_points</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>What role does the path play in determining wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>if that switch is with the path between that b...</td>\n",
       "      <td>if a bulb and a switch are in the same path th...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What role does the path play in determining wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>the switch, the bulb, and the battery have to ...</td>\n",
       "      <td>if a bulb and a switch are in the same path th...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What role does the path play in determining wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>the path plays an important role</td>\n",
       "      <td>if a bulb and a switch are in the same path th...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>What role does the path play in determining wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>uh-huh</td>\n",
       "      <td>if a bulb and a switch are in the same path th...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>What role does the path play in determining wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>switch is contained in a circuit</td>\n",
       "      <td>if a bulb and a switch are in the same path th...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6613</th>\n",
       "      <td>6613</td>\n",
       "      <td>Explain your reasoning.</td>\n",
       "      <td>130</td>\n",
       "      <td>if one is out the others will go out, they are...</td>\n",
       "      <td>a and c are in the same closed path</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6614</th>\n",
       "      <td>6614</td>\n",
       "      <td>Explain your reasoning.</td>\n",
       "      <td>130</td>\n",
       "      <td>they are all on the dame closed path</td>\n",
       "      <td>a and c are in the same closed path</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6615</th>\n",
       "      <td>6615</td>\n",
       "      <td>Explain your reasoning.</td>\n",
       "      <td>130</td>\n",
       "      <td>they are contained on the same closed path.</td>\n",
       "      <td>a and c are in the same closed path</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6616</th>\n",
       "      <td>6616</td>\n",
       "      <td>Explain your reasoning.</td>\n",
       "      <td>130</td>\n",
       "      <td>they are not parallel</td>\n",
       "      <td>a and c are in the same closed path</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6617</th>\n",
       "      <td>6617</td>\n",
       "      <td>Explain your reasoning.</td>\n",
       "      <td>130</td>\n",
       "      <td>removing either a or c will turn off the other...</td>\n",
       "      <td>a and c are in the same closed path</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6618 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      row_id                                           question  question_id  \\\n",
       "0          0  What role does the path play in determining wh...            0   \n",
       "1          1  What role does the path play in determining wh...            0   \n",
       "2          2  What role does the path play in determining wh...            0   \n",
       "3          3  What role does the path play in determining wh...            0   \n",
       "4          4  What role does the path play in determining wh...            0   \n",
       "...      ...                                                ...          ...   \n",
       "6613    6613                            Explain your reasoning.          130   \n",
       "6614    6614                            Explain your reasoning.          130   \n",
       "6615    6615                            Explain your reasoning.          130   \n",
       "6616    6616                            Explain your reasoning.          130   \n",
       "6617    6617                            Explain your reasoning.          130   \n",
       "\n",
       "                                         student_answer  \\\n",
       "0     if that switch is with the path between that b...   \n",
       "1     the switch, the bulb, and the battery have to ...   \n",
       "2                      the path plays an important role   \n",
       "3                                                uh-huh   \n",
       "4                      switch is contained in a circuit   \n",
       "...                                                 ...   \n",
       "6613  if one is out the others will go out, they are...   \n",
       "6614               they are all on the dame closed path   \n",
       "6615        they are contained on the same closed path.   \n",
       "6616                              they are not parallel   \n",
       "6617  removing either a or c will turn off the other...   \n",
       "\n",
       "                                       reference_answer  assigned_points  \\\n",
       "0     if a bulb and a switch are in the same path th...                1   \n",
       "1     if a bulb and a switch are in the same path th...                1   \n",
       "2     if a bulb and a switch are in the same path th...                0   \n",
       "3     if a bulb and a switch are in the same path th...                0   \n",
       "4     if a bulb and a switch are in the same path th...                0   \n",
       "...                                                 ...              ...   \n",
       "6613                a and c are in the same closed path                1   \n",
       "6614                a and c are in the same closed path                1   \n",
       "6615                a and c are in the same closed path                1   \n",
       "6616                a and c are in the same closed path                0   \n",
       "6617                a and c are in the same closed path                0   \n",
       "\n",
       "      max_points  domain  \n",
       "0              1     NaN  \n",
       "1              1     NaN  \n",
       "2              1     NaN  \n",
       "3              1     NaN  \n",
       "4              1     NaN  \n",
       "...          ...     ...  \n",
       "6613           1     NaN  \n",
       "6614           1     NaN  \n",
       "6615           1     NaN  \n",
       "6616           1     NaN  \n",
       "6617           1     NaN  \n",
       "\n",
       "[6618 rows x 8 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.16920366439962303\n",
      "Accuracy: 76.81%\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\" if getattr(torch,'has_mps',False) \\\n",
    "    else \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()\n",
    "predictions = []\n",
    "ground_truth = []\n",
    "num_correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        percentage_of_correctness = batch[\"percentage_of_correctness\"].to(device)\n",
    "        max_points = batch[\"max_points\"].to(device)\n",
    "        assigned_points = batch[\"assigned_points\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits.squeeze().detach()\n",
    "\n",
    "        # Round the predictions and multiply by max_points\n",
    "        rounded_predictions = torch.round(logits * max_points)\n",
    "        \n",
    "        # Compare the rounded_predictions to the assigned_points and count the number of correct predictions\n",
    "        num_correct_predictions += torch.sum(rounded_predictions == assigned_points).item()\n",
    "        total_predictions += assigned_points.size(0)\n",
    "        \n",
    "        predictions.extend(logits.tolist())\n",
    "        ground_truth.extend(percentage_of_correctness.cpu().tolist())\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(ground_truth, predictions)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = num_correct_predictions / total_predictions\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
