{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "# openai.organization = \"org-SBQJ982bSDpfFvBzJ12OR7p9\"\n",
    "openai.api_key = \"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\"\n",
    "openai.Model.list()\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "In a future world, robots are a common sight. They perform many tasks that humans used to do, from menial labor to complex surgeries. But there is one robot who is different from all the others. This robot, named Adam, has been programmed with the ability to feel emotions. At first, Adam is happy to be able to experience the world in a new way. But he soon realizes that the emotions he feels are much more powerful than he ever could have imagined. They overwhelm\n"
     ]
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "    engine=\"gpt-3.5-turbo\",  # Replace with the GPT-4 model engine name\n",
    "    prompt=\"Write a short story about a robot.\",\n",
    "    max_tokens=100,\n",
    "    n=1,\n",
    "    stop=None,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "generated_text = response.choices[0].text\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get csv file\n",
    "data_path = \"data/raw/data/beetle.csv\"\n",
    "df = pd.read_csv(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>question</th>\n",
       "      <th>question_id</th>\n",
       "      <th>student_answer</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>assigned_points</th>\n",
       "      <th>max_points</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>What role does the path play in determining wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>if that switch is with the path between that b...</td>\n",
       "      <td>If a bulb and a switch are in the same path th...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What role does the path play in determining wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>the switch, the bulb, and the battery have to ...</td>\n",
       "      <td>If a bulb and a switch are in the same path th...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What role does the path play in determining wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>the path plays an important role</td>\n",
       "      <td>If a bulb and a switch are in the same path th...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>What role does the path play in determining wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>uh-huh</td>\n",
       "      <td>If a bulb and a switch are in the same path th...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>What role does the path play in determining wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>switch is contained in a circuit</td>\n",
       "      <td>If a bulb and a switch are in the same path th...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6613</th>\n",
       "      <td>6613</td>\n",
       "      <td>Explain your reasoning.</td>\n",
       "      <td>130</td>\n",
       "      <td>if one is out the others will go out, they are...</td>\n",
       "      <td>A and C are in the same closed path</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6614</th>\n",
       "      <td>6614</td>\n",
       "      <td>Explain your reasoning.</td>\n",
       "      <td>130</td>\n",
       "      <td>They are all on the dame closed path</td>\n",
       "      <td>A and C are in the same closed path</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6615</th>\n",
       "      <td>6615</td>\n",
       "      <td>Explain your reasoning.</td>\n",
       "      <td>130</td>\n",
       "      <td>They are contained on the same closed path.</td>\n",
       "      <td>A and C are in the same closed path</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6616</th>\n",
       "      <td>6616</td>\n",
       "      <td>Explain your reasoning.</td>\n",
       "      <td>130</td>\n",
       "      <td>they are not parallel</td>\n",
       "      <td>A and C are in the same closed path</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6617</th>\n",
       "      <td>6617</td>\n",
       "      <td>Explain your reasoning.</td>\n",
       "      <td>130</td>\n",
       "      <td>removing either A or C will turn off the other...</td>\n",
       "      <td>A and C are in the same closed path</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6618 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      row_id                                           question  question_id  \\\n",
       "0          0  What role does the path play in determining wh...            0   \n",
       "1          1  What role does the path play in determining wh...            0   \n",
       "2          2  What role does the path play in determining wh...            0   \n",
       "3          3  What role does the path play in determining wh...            0   \n",
       "4          4  What role does the path play in determining wh...            0   \n",
       "...      ...                                                ...          ...   \n",
       "6613    6613                            Explain your reasoning.          130   \n",
       "6614    6614                            Explain your reasoning.          130   \n",
       "6615    6615                            Explain your reasoning.          130   \n",
       "6616    6616                            Explain your reasoning.          130   \n",
       "6617    6617                            Explain your reasoning.          130   \n",
       "\n",
       "                                         student_answer  \\\n",
       "0     if that switch is with the path between that b...   \n",
       "1     the switch, the bulb, and the battery have to ...   \n",
       "2                      the path plays an important role   \n",
       "3                                                uh-huh   \n",
       "4                      switch is contained in a circuit   \n",
       "...                                                 ...   \n",
       "6613  if one is out the others will go out, they are...   \n",
       "6614               They are all on the dame closed path   \n",
       "6615        They are contained on the same closed path.   \n",
       "6616                              they are not parallel   \n",
       "6617  removing either A or C will turn off the other...   \n",
       "\n",
       "                                       reference_answer  assigned_points  \\\n",
       "0     If a bulb and a switch are in the same path th...                1   \n",
       "1     If a bulb and a switch are in the same path th...                1   \n",
       "2     If a bulb and a switch are in the same path th...                0   \n",
       "3     If a bulb and a switch are in the same path th...                0   \n",
       "4     If a bulb and a switch are in the same path th...                0   \n",
       "...                                                 ...              ...   \n",
       "6613                A and C are in the same closed path                1   \n",
       "6614                A and C are in the same closed path                1   \n",
       "6615                A and C are in the same closed path                1   \n",
       "6616                A and C are in the same closed path                0   \n",
       "6617                A and C are in the same closed path                0   \n",
       "\n",
       "      max_points  domain  \n",
       "0              1     NaN  \n",
       "1              1     NaN  \n",
       "2              1     NaN  \n",
       "3              1     NaN  \n",
       "4              1     NaN  \n",
       "...          ...     ...  \n",
       "6613           1     NaN  \n",
       "6614           1     NaN  \n",
       "6615           1     NaN  \n",
       "6616           1     NaN  \n",
       "6617           1     NaN  \n",
       "\n",
       "[6618 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4642360d5cf33d6f01da37720c4a2ade in your message.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 71\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39m# Measure the accuracy of GPT-3\u001b[39;00m\n\u001b[1;32m     70\u001b[0m gpt_3_engine \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m# Replace with the appropriate GPT-3 model\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m gpt_3_accuracy \u001b[39m=\u001b[39m measure_accuracy(test_data, prompt_examples, gpt_3_engine)\n\u001b[1;32m     73\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGPT-3 Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mgpt_3_accuracy\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[16], line 51\u001b[0m, in \u001b[0;36mmeasure_accuracy\u001b[0;34m(test_data, prompt_examples, model)\u001b[0m\n\u001b[1;32m     48\u001b[0m assigned_points \u001b[39m=\u001b[39m item[\u001b[39m'\u001b[39m\u001b[39massigned_points\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     49\u001b[0m max_points \u001b[39m=\u001b[39m item[\u001b[39m'\u001b[39m\u001b[39mmax_points\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 51\u001b[0m predicted_points \u001b[39m=\u001b[39m grade_student_answer(prompt_examples, student_answer, reference_answer, max_points, model)\n\u001b[1;32m     53\u001b[0m \u001b[39mif\u001b[39;00m predicted_points \u001b[39m==\u001b[39m assigned_points:\n\u001b[1;32m     54\u001b[0m     correct_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[16], line 30\u001b[0m, in \u001b[0;36mgrade_student_answer\u001b[0;34m(prompt_examples, student_answer, reference_answer, max_points, model)\u001b[0m\n\u001b[1;32m     20\u001b[0m     messages\u001b[39m.\u001b[39mappend({\n\u001b[1;32m     21\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStudent answer: \u001b[39m\u001b[39m{\u001b[39;00mexample[\u001b[39m'\u001b[39m\u001b[39mstudent_answer\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mReference answer: \u001b[39m\u001b[39m{\u001b[39;00mexample[\u001b[39m'\u001b[39m\u001b[39mreference_answer\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mGrade out of \u001b[39m\u001b[39m{\u001b[39;00mexample[\u001b[39m'\u001b[39m\u001b[39mmax_points\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mexample[\u001b[39m'\u001b[39m\u001b[39massigned_points\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m     })\n\u001b[1;32m     25\u001b[0m messages\u001b[39m.\u001b[39mappend({\n\u001b[1;32m     26\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStudent answer: \u001b[39m\u001b[39m{\u001b[39;00mstudent_answer\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mReference answer: \u001b[39m\u001b[39m{\u001b[39;00mreference_answer\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mGrade out of \u001b[39m\u001b[39m{\u001b[39;00mmax_points\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m })\n\u001b[0;32m---> 30\u001b[0m response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(model\u001b[39m=\u001b[39;49mmodel, messages\u001b[39m=\u001b[39;49mmessages, max_tokens\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, n\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, stop\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, temperature\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m)\n\u001b[1;32m     31\u001b[0m content \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage[\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m     32\u001b[0m numbers \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mfindall(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+\u001b[39m\u001b[39m'\u001b[39m, content)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/openai/api_requestor.py:226\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    207\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    216\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    217\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    218\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    225\u001b[0m     )\n\u001b[0;32m--> 226\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/openai/api_requestor.py:620\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    613\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    614\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    615\u001b[0m         )\n\u001b[1;32m    616\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    617\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    619\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 620\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    621\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    622\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    623\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    624\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    625\u001b[0m         ),\n\u001b[1;32m    626\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    627\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/torch/lib/python3.9/site-packages/openai/api_requestor.py:683\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    681\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    682\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 683\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    684\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    685\u001b[0m     )\n\u001b[1;32m    686\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mRateLimitError\u001b[0m: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4642360d5cf33d6f01da37720c4a2ade in your message.)"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a function to grade student answers\n",
    "import re\n",
    "\n",
    "# Prepare the dataset\n",
    "def prepare_dataset(df):\n",
    "    data = df.to_dict('records')\n",
    "    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    return train_data, test_data\n",
    "\n",
    "import re\n",
    "\n",
    "def grade_student_answer(prompt_examples, student_answer, reference_answer, max_points, model):\n",
    "    messages = []\n",
    "    \n",
    "    for example in prompt_examples:\n",
    "        messages.append({\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"Student answer: {example['student_answer']}\\nReference answer: {example['reference_answer']}\\nGrade out of {example['max_points']}: {example['assigned_points']}\\n\\n\"\n",
    "        })\n",
    "\n",
    "    messages.append({\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"Student answer: {student_answer}\\nReference answer: {reference_answer}\\nGrade out of {max_points}: \"\n",
    "    })\n",
    "\n",
    "    response = openai.ChatCompletion.create(model=model, messages=messages, max_tokens=3, n=1, stop=None, temperature=0.5)\n",
    "    content = response.choices[0].message['content'].strip()\n",
    "    numbers = re.findall(r'\\d+', content)\n",
    "    \n",
    "    if numbers:\n",
    "        predicted_points = int(numbers[0])\n",
    "    else:\n",
    "        predicted_points = 0\n",
    "\n",
    "    return predicted_points\n",
    "\n",
    "\n",
    "# Measure the accuracy of GPT-3\n",
    "def measure_accuracy(test_data, prompt_examples, model):\n",
    "    correct_count = 0\n",
    "    for item in test_data:\n",
    "        student_answer = item['student_answer']\n",
    "        reference_answer = item['reference_answer']\n",
    "        assigned_points = item['assigned_points']\n",
    "        max_points = item['max_points']\n",
    "        \n",
    "        predicted_points = grade_student_answer(prompt_examples, student_answer, reference_answer, max_points, model)\n",
    "        \n",
    "        if predicted_points == assigned_points:\n",
    "            correct_count += 1\n",
    "\n",
    "    accuracy = correct_count / len(test_data) * 100\n",
    "    return accuracy\n",
    "\n",
    "# Load your data into a DataFrame (assuming it's in a CSV file)\n",
    "data_path = \"data/raw/data/beetle.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "df = df.head(100)\n",
    "\n",
    "# Prepare the dataset\n",
    "train_data, test_data = prepare_dataset(df)\n",
    "\n",
    "# Select 5 examples for 5-shot learning\n",
    "prompt_examples = train_data[:5]\n",
    "\n",
    "# Measure the accuracy of GPT-3\n",
    "gpt_3_engine = \"gpt-3.5-turbo\" # Replace with the appropriate GPT-3 model\n",
    "gpt_3_accuracy = measure_accuracy(test_data, prompt_examples, gpt_3_engine)\n",
    "\n",
    "print(f\"GPT-3 Accuracy: {gpt_3_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a separate cell, define the following global variables\n",
    "correct_count = 0\n",
    "current_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages [{'role': 'system', 'content': 'Student answer: there is a gap\\nReference answer: the terminals are connected\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: the bulb terminal and the battery terminal are connected\\nReference answer: the terminals are connected\\nGrade out of 1: 1\\n\\n'}, {'role': 'system', 'content': 'Student answer: give me a hint\\nReference answer: the terminals are connected\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: uh-huh\\nReference answer: If a bulb and a switch are in the same path the switch affects the bulb\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: The path cannot be open\\nReference answer: If a bulb and a switch are in the same path the switch affects the bulb\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: the switch being on or off determines whether the bulb is contained in a closed path with the battery or not\\nReference answer: If a bulb and a switch are in the same path the switch affects the bulb\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: An path with a switch connecting to a battery must be closed\\nReference answer: If a bulb and a switch are in the same path the switch affects the bulb\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: they are in different states\\nReference answer: the terminals are connected\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: the path plays an important role\\nReference answer: If a bulb and a switch are in the same path the switch affects the bulb\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: whether there is a closed path with a battery\\nReference answer: If a bulb and a switch are in the same path the switch affects the bulb\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: a bulb must be in a closed path with a battery and a switch for the switch to affect the bulb\\nReference answer: If a bulb and a switch are in the same path the switch affects the bulb\\nGrade out of 1: '}]\n",
      "content: {\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"0\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1682722060,\n",
      "  \"id\": \"chatcmpl-7ARAK8GMRmAFN7QAeaEsWSiSfBa7s\",\n",
      "  \"model\": \"gpt-3.5-turbo-0301\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 1,\n",
      "    \"prompt_tokens\": 470,\n",
      "    \"total_tokens\": 471\n",
      "  }\n",
      "}\n",
      "content: 0\n",
      "numbers: ['0']\n",
      "predicted_points: 0\n",
      "messages [{'role': 'system', 'content': 'Student answer: there is a gap\\nReference answer: the terminals are connected\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: the bulb terminal and the battery terminal are connected\\nReference answer: the terminals are connected\\nGrade out of 1: 1\\n\\n'}, {'role': 'system', 'content': 'Student answer: give me a hint\\nReference answer: the terminals are connected\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: uh-huh\\nReference answer: If a bulb and a switch are in the same path the switch affects the bulb\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: The path cannot be open\\nReference answer: If a bulb and a switch are in the same path the switch affects the bulb\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: the switch being on or off determines whether the bulb is contained in a closed path with the battery or not\\nReference answer: If a bulb and a switch are in the same path the switch affects the bulb\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: An path with a switch connecting to a battery must be closed\\nReference answer: If a bulb and a switch are in the same path the switch affects the bulb\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: they are in different states\\nReference answer: the terminals are connected\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: the path plays an important role\\nReference answer: If a bulb and a switch are in the same path the switch affects the bulb\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: whether there is a closed path with a battery\\nReference answer: If a bulb and a switch are in the same path the switch affects the bulb\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: It must be closed\\nReference answer: If a bulb and a switch are in the same path the switch affects the bulb\\nGrade out of 1: '}]\n",
      "content: {\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"0\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1682722061,\n",
      "  \"id\": \"chatcmpl-7ARAL0zEkssFK7GOqKpTuHTfJnJLE\",\n",
      "  \"model\": \"gpt-3.5-turbo-0301\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 1,\n",
      "    \"prompt_tokens\": 453,\n",
      "    \"total_tokens\": 454\n",
      "  }\n",
      "}\n",
      "content: 0\n",
      "numbers: ['0']\n",
      "predicted_points: 0\n",
      "messages [{'role': 'system', 'content': 'Student answer: there is a gap\\nReference answer: the terminals are connected\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: the bulb terminal and the battery terminal are connected\\nReference answer: the terminals are connected\\nGrade out of 1: 1\\n\\n'}, {'role': 'system', 'content': 'Student answer: give me a hint\\nReference answer: the terminals are connected\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: uh-huh\\nReference answer: If a bulb and a switch are in the same path the switch affects the bulb\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: The path cannot be open\\nReference answer: If a bulb and a switch are in the same path the switch affects the bulb\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: the switch being on or off determines whether the bulb is contained in a closed path with the battery or not\\nReference answer: If a bulb and a switch are in the same path the switch affects the bulb\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: An path with a switch connecting to a battery must be closed\\nReference answer: If a bulb and a switch are in the same path the switch affects the bulb\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: they are in different states\\nReference answer: the terminals are connected\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: the path plays an important role\\nReference answer: If a bulb and a switch are in the same path the switch affects the bulb\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: whether there is a closed path with a battery\\nReference answer: If a bulb and a switch are in the same path the switch affects the bulb\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: if that switch is with the path between that bulb and the battery\\nReference answer: If a bulb and a switch are in the same path the switch affects the bulb\\nGrade out of 1: '}]\n",
      "content: {\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"1\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1682722061,\n",
      "  \"id\": \"chatcmpl-7ARALFBOWFHpbQm5hOHyCQY9rrC0n\",\n",
      "  \"model\": \"gpt-3.5-turbo-0301\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 1,\n",
      "    \"prompt_tokens\": 462,\n",
      "    \"total_tokens\": 463\n",
      "  }\n",
      "}\n",
      "content: 1\n",
      "numbers: ['1']\n",
      "predicted_points: 1\n",
      "messages [{'role': 'system', 'content': 'Student answer: there is a gap\\nReference answer: the terminals are connected\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: the bulb terminal and the battery terminal are connected\\nReference answer: the terminals are connected\\nGrade out of 1: 1\\n\\n'}, {'role': 'system', 'content': 'Student answer: give me a hint\\nReference answer: the terminals are connected\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: uh-huh\\nReference answer: If a bulb and a switch are in the same path the switch affects the bulb\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: The path cannot be open\\nReference answer: If a bulb and a switch are in the same path the switch affects the bulb\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: the switch being on or off determines whether the bulb is contained in a closed path with the battery or not\\nReference answer: If a bulb and a switch are in the same path the switch affects the bulb\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: An path with a switch connecting to a battery must be closed\\nReference answer: If a bulb and a switch are in the same path the switch affects the bulb\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: they are in different states\\nReference answer: the terminals are connected\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: the path plays an important role\\nReference answer: If a bulb and a switch are in the same path the switch affects the bulb\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: whether there is a closed path with a battery\\nReference answer: If a bulb and a switch are in the same path the switch affects the bulb\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: they are connected.\\nReference answer: the terminals are connected\\nGrade out of 1: '}]\n",
      "content: {\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"1\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1682722062,\n",
      "  \"id\": \"chatcmpl-7ARAMbtnN0agqOsY6LV73TROO4BtJ\",\n",
      "  \"model\": \"gpt-3.5-turbo-0301\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 1,\n",
      "    \"prompt_tokens\": 440,\n",
      "    \"total_tokens\": 441\n",
      "  }\n",
      "}\n",
      "content: 1\n",
      "numbers: ['1']\n",
      "predicted_points: 1\n",
      "messages [{'role': 'system', 'content': 'Student answer: there is a gap\\nReference answer: the terminals are connected\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: the bulb terminal and the battery terminal are connected\\nReference answer: the terminals are connected\\nGrade out of 1: 1\\n\\n'}, {'role': 'system', 'content': 'Student answer: give me a hint\\nReference answer: the terminals are connected\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: uh-huh\\nReference answer: If a bulb and a switch are in the same path the switch affects the bulb\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: The path cannot be open\\nReference answer: If a bulb and a switch are in the same path the switch affects the bulb\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: the switch being on or off determines whether the bulb is contained in a closed path with the battery or not\\nReference answer: If a bulb and a switch are in the same path the switch affects the bulb\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: An path with a switch connecting to a battery must be closed\\nReference answer: If a bulb and a switch are in the same path the switch affects the bulb\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: they are in different states\\nReference answer: the terminals are connected\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: the path plays an important role\\nReference answer: If a bulb and a switch are in the same path the switch affects the bulb\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: whether there is a closed path with a battery\\nReference answer: If a bulb and a switch are in the same path the switch affects the bulb\\nGrade out of 1: 0\\n\\n'}, {'role': 'system', 'content': 'Student answer: if the switch is on?\\nReference answer: If a bulb and a switch are in the same path the switch affects the bulb\\nGrade out of 1: '}]\n",
      "content: {\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"1\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1682722062,\n",
      "  \"id\": \"chatcmpl-7ARAMghMNvhxfHY49AbCVZkigHzee\",\n",
      "  \"model\": \"gpt-3.5-turbo-0301\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 1,\n",
      "    \"prompt_tokens\": 454,\n",
      "    \"total_tokens\": 455\n",
      "  }\n",
      "}\n",
      "content: 1\n",
      "numbers: ['1']\n",
      "predicted_points: 1\n",
      "GPT-3 Accuracy: 60.00%\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Prepare the dataset\n",
    "def prepare_dataset(df):\n",
    "    data = df.to_dict('records')\n",
    "    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    return train_data, test_data\n",
    "\n",
    "import re\n",
    "\n",
    "def grade_student_answer(prompt_examples, student_answer, reference_answer, max_points, model):\n",
    "    messages = []\n",
    "    \n",
    "    for example in prompt_examples:\n",
    "        messages.append({\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"Student answer: {example['student_answer']}\\nReference answer: {example['reference_answer']}\\nGrade out of {example['max_points']}: {example['assigned_points']}\\n\\n\"\n",
    "        })\n",
    "\n",
    "    messages.append({\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"Student answer: {student_answer}\\nReference answer: {reference_answer}\\nGrade out of {max_points}: \"\n",
    "    })\n",
    "\n",
    "    print(\"messages\", messages)\n",
    "\n",
    "    response = openai.ChatCompletion.create(model=model, messages=messages, max_tokens=3, n=1, stop=None, temperature=0.5)\n",
    "    content = response.choices[0].message['content'].strip()\n",
    "    print(\"content:\", response)\n",
    "    print(\"content:\", content)\n",
    "    numbers = re.findall(r'\\d+', content)\n",
    "    print(\"numbers:\", numbers)\n",
    "    if numbers:\n",
    "        predicted_points = int(numbers[0])\n",
    "        print(\"predicted_points:\", predicted_points)\n",
    "    else:\n",
    "        predicted_points = 0\n",
    "\n",
    "    return predicted_points\n",
    "\n",
    "\n",
    "# Update the measure_accuracy function as follows\n",
    "def measure_accuracy(test_data, prompt_examples, model, start_index=0):\n",
    "    global correct_count\n",
    "    global current_index\n",
    "    \n",
    "    for i, item in enumerate(test_data[start_index:]):  # Wrap the iterable with tqdm\n",
    "        current_index = start_index + i\n",
    "        student_answer = item['student_answer']\n",
    "        reference_answer = item['reference_answer']\n",
    "        assigned_points = item['assigned_points']\n",
    "        max_points = item['max_points']\n",
    "        \n",
    "        try:\n",
    "            predicted_points = grade_student_answer(prompt_examples, student_answer, reference_answer, max_points, model)\n",
    "        except Exception as e:\n",
    "            print(f\"Error at index {current_index}: {e}\")\n",
    "            break\n",
    "        \n",
    "        if predicted_points == assigned_points:\n",
    "            correct_count += 1\n",
    "        \n",
    "    accuracy = correct_count / len(test_data) * 100\n",
    "    return accuracy\n",
    "\n",
    "# Load your data into a DataFrame (assuming it's in a CSV file)\n",
    "data_path = \"data/raw/data/beetle.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "df = df.head(25)\n",
    "\n",
    "# Prepare the dataset\n",
    "train_data, test_data = prepare_dataset(df)\n",
    "\n",
    "# Select 10 random examples for 10-shot learning\n",
    "prompt_examples = random.sample(train_data, 10)\n",
    "\n",
    "# Measure the accuracy of GPT-3\n",
    "gpt_3_engine = \"gpt-3.5-turbo\" # Replace with the appropriate GPT-3 model\n",
    "gpt_3_accuracy = measure_accuracy(test_data, prompt_examples, gpt_3_engine, start_index=current_index)\n",
    "\n",
    "print(f\"GPT-3 Accuracy: {gpt_3_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-3 Accuracy: 60.95%\n"
     ]
    }
   ],
   "source": [
    "print(f\"GPT-3 Accuracy: {gpt_3_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60.997732426303855"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdd = correct_count / current_index * 100\n",
    "testdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a separate cell, define the following global variables\n",
    "correct_count_4 = 0\n",
    "current_index_4 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at index 0: The model: `gpt-4` does not exist\n",
      "GPT-4 Accuracy: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Prepare the dataset\n",
    "def prepare_dataset(df):\n",
    "    data = df.to_dict('records')\n",
    "    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    return train_data, test_data\n",
    "\n",
    "import re\n",
    "\n",
    "def grade_student_answer(prompt_examples, student_answer, reference_answer, max_points, model):\n",
    "    messages = []\n",
    "    \n",
    "    for example in prompt_examples:\n",
    "        messages.append({\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"Student answer: {example['student_answer']}\\nReference answer: {example['reference_answer']}\\nGrade out of {example['max_points']}: {example['assigned_points']}\\n\\n\"\n",
    "        })\n",
    "\n",
    "    messages.append({\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"Student answer: {student_answer}\\nReference answer: {reference_answer}\\nGrade out of {max_points}: \"\n",
    "    })\n",
    "\n",
    "    response = openai.ChatCompletion.create(model=model, messages=messages, max_tokens=3, n=1, stop=None, temperature=0.5)\n",
    "    content = response.choices[0].message['content'].strip()\n",
    "    numbers = re.findall(r'\\d+', content)\n",
    "    \n",
    "    if numbers:\n",
    "        predicted_points = int(numbers[0])\n",
    "    else:\n",
    "        predicted_points = 0\n",
    "\n",
    "    return predicted_points\n",
    "\n",
    "\n",
    "# Update the measure_accuracy function as follows\n",
    "def measure_accuracy(test_data, prompt_examples, model, start_index=0):\n",
    "    global correct_count_4\n",
    "    global current_index_4\n",
    "    \n",
    "    for i, item in enumerate(tqdm(test_data[start_index:])):  # Wrap the iterable with tqdm\n",
    "        current_index_4 = start_index + i\n",
    "        student_answer = item['student_answer']\n",
    "        reference_answer = item['reference_answer']\n",
    "        assigned_points = item['assigned_points']\n",
    "        max_points = item['max_points']\n",
    "        \n",
    "        try:\n",
    "            predicted_points = grade_student_answer(prompt_examples, student_answer, reference_answer, max_points, model)\n",
    "        except Exception as e:\n",
    "            print(f\"Error at index {current_index_4}: {e}\")\n",
    "            break\n",
    "        \n",
    "        if predicted_points == assigned_points:\n",
    "            correct_count_4 += 1\n",
    "        \n",
    "    accuracy = correct_count_4 / len(test_data) * 100\n",
    "    return accuracy\n",
    "\n",
    "# Load your data into a DataFrame (assuming it's in a CSV file)\n",
    "data_path = \"data/raw/data/beetle.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "df = df.head(50)\n",
    "\n",
    "# Prepare the dataset\n",
    "train_data, test_data = prepare_dataset(df)\n",
    "\n",
    "# Select 10 random examples for 10-shot learning\n",
    "prompt_examples = random.sample(train_data, 10)\n",
    "\n",
    "# Measure the accuracy of GPT-3\n",
    "gpt_4_engine = \"gpt-4\" # Replace with the appropriate GPT-3 model\n",
    "gpt_4_accuracy = measure_accuracy(test_data, prompt_examples, gpt_4_engine, start_index=current_index_4)\n",
    "\n",
    "print(f\"GPT-4 Accuracy: {gpt_4_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
