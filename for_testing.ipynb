{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found neural_course with type .parquet in (data_saved/emebed_words_conceptnet) and converted it to df\n",
      "df found: True, name: emebed_words_conceptnet\n",
      "File neural_course not found in (data_saved/conceptnet_cosine_similarity)\n",
      "df found: False, name: conceptnet_cosine_similarity\n",
      "No need to run emebed_words_conceptnet for neural_course\n",
      "Running conceptnet_cosine_similarity for neural_course\n",
      "basic processing starting for neural_course\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/646 [00:00<?, ?it/s]/Users/olaf/opt/miniconda3/envs/torch/lib/python3.9/site-packages/scipy/spatial/distance.py:620: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "100%|██████████| 646/646 [00:00<00:00, 30564.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no saving needed because basic emebed_words_conceptnet already done on neural_course\n",
      "saving new conceptnet_cosine_similarity phase for: neural_course\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from classes.Dataset_Basic import Dataset_Basic\n",
    "from classes.Process_Stages import Process_Stages\n",
    "\n",
    "from classes.Dataset_Settings import Dataset_Settings\n",
    "\n",
    "from run_models.cosine_sililarity.classes.Dataset_Cosine import Dataset_Cosine\n",
    "from run_models.gensim.classes.Process_Stages_Gensim import Process_Stages_Gensim\n",
    "\n",
    "from run_models.embed_words.classes.Embed_Words import Embed_Words\n",
    "\n",
    "from run_models.gensim.services.gensim_services import gensim_download, gensim_save, gensim_load\n",
    "from word_embedding.models.classes.EmbeddingModel import EmbeddingModel\n",
    "\n",
    "name_dataset = \"conceptnet_cosine_similarity\"\n",
    "\n",
    "datasets = {}\n",
    "datasets[\"neural_course\"] = Dataset_Cosine(\n",
    "    df_name=\"neural_course\",\n",
    "    model_name=name_dataset, # used for dir name inside data_saved\n",
    "\n",
    "    language=\"english\",\n",
    "\n",
    "    datasets = {\n",
    "        \"emebed_words_conceptnet\": Dataset_Settings(\n",
    "            df=None,\n",
    "            may_run_now=False,\n",
    "            required=True,\n",
    "            parquet=True\n",
    "        ),\n",
    "        name_dataset: Dataset_Settings(\n",
    "            df=None,\n",
    "            may_run_now=True,\n",
    "            required=True,\n",
    "            parquet=True,\n",
    "            name_required_dataset=\"gensim\"\n",
    "        ),\n",
    "    },\n",
    "\n",
    "    columns_to_add = {name_dataset: {\"cosine_similarity\": []}},\n",
    "\n",
    ")\n",
    "\n",
    "# get dataset, process dataset, save dataset\n",
    "datasets[\"neural_course\"].get_dataset()\n",
    "datasets[\"neural_course\"].process_dataset()\n",
    "datasets[\"neural_course\"].add_columns()\n",
    "datasets[\"neural_course\"].save()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Initialize the tokenizer and the model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Your sentence\n",
    "sentence = \"This is a simple sentence.\"\n",
    "\n",
    "# Tokenize the sentence and encode it to IDs\n",
    "input_ids = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "\n",
    "# Create tensors\n",
    "input_ids = torch.tensor([input_ids])\n",
    "\n",
    "# Forward pass: compute the BERT embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "\n",
    "# Extract the last hidden states (the embeddings)\n",
    "last_hidden_states = outputs[0]\n",
    "\n",
    "# # Print the embeddings\n",
    "# for i, token_str in enumerate(tokenizer.convert_ids_to_tokens(input_ids[0])):\n",
    "#     print(token_str, last_hidden_states[0, i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence):\n",
    "    # Tokenize the sentence and encode it to IDs\n",
    "    input_ids = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "\n",
    "    # Create tensors\n",
    "    input_ids = torch.tensor([input_ids])\n",
    "\n",
    "    # Forward pass: compute the BERT embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "    # Extract the last hidden states (the embeddings)\n",
    "    last_hidden_states = outputs[0]\n",
    "\n",
    "    return last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found neural_course with type parquet in (data/embed_words/data/BERT/data) and converted it to df\n",
      "df found: True, name: BERT\n",
      "No need to run standardized_splits for neural_course\n",
      "No need to run BERT for neural_course\n",
      "no saving needed because basic BERT already done on neural_course\n"
     ]
    }
   ],
   "source": [
    "# classes\n",
    "from data.embed_words.BERT_embedding.classes.BERT_Embedding import BERT_Embedding\n",
    "from classes.Dataset_Settings import Dataset_Settings\n",
    "\n",
    "BERT_embed = BERT_Embedding(\n",
    "    df_name=\"neural_course\",\n",
    "    model_name=\"BERT\", # used for dir name inside data_saved\n",
    "\n",
    "    language=\"english\",\n",
    "\n",
    "    datasets = {\n",
    "        \"standardized_splits\": Dataset_Settings(\n",
    "            df=None,\n",
    "            df_name=\"splits\",\n",
    "            base_dir=\"data\",\n",
    "\n",
    "            may_run_now=False,\n",
    "            required=True,\n",
    "        ),\n",
    "        \"BERT\": Dataset_Settings(\n",
    "            df=None,\n",
    "            df_name=\"BERT\",\n",
    "            base_dir=\"data/embed_words/data\",\n",
    "\n",
    "            may_run_now=True,\n",
    "            required=True,\n",
    "            parquet=True,\n",
    "            name_required_dataset=\"standardized_splits\",\n",
    "            force_run=False\n",
    "        ),\n",
    "    },\n",
    "\n",
    "    pre_trained_BERT_model_name=\"bert-base-uncased\"\n",
    "\n",
    ")\n",
    "\n",
    "BERT_embed.run_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Marly is lief\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed a sentence in BERT_tokenized\n",
    "import pandas as pd\n",
    "\n",
    "df_BERT = pd.read_parquet(\"data/embed_words/data/BERT/data/neural_course.parquet\")\n",
    "df_2 = pd.read_csv(\"data/splits/data/neural_course.csv\")\n",
    "fast_text = pd.read_parquet(\"data/embed_words/data/fasttext/data/neural_course.parquet\")\n",
    "\n",
    "basic = pd.read_csv(\"data/basic_processed/data/neural_course.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from services.string_array import str_to_array\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "1 - cosine(str_to_array(df_BERT.iloc[0][\"student_answer\"])[0], str_to_array(df_BERT.iloc[0][\"student_answer\"])[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.read_csv(\"performance_tracking/tracking/performance_tracking.csv\")\n",
    "performance = performance.query(\"dataset_split == 'validation_df'\")\n",
    "performance = performance.query(\"dataset_name != 'concatenated_datasets'\")\n",
    "performance = performance.query(\"dataset_name != 'concatenated_domains'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>embedding_seperated</th>\n",
       "      <th>embedding_model_name</th>\n",
       "      <th>classification_model_name</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>dataset_split</th>\n",
       "      <th>seed_data_split</th>\n",
       "      <th>time_stamp</th>\n",
       "      <th>rmse</th>\n",
       "      <th>pears_correlation</th>\n",
       "      <th>...</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>precision_micro</th>\n",
       "      <th>recall_micro</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>precision_weighted</th>\n",
       "      <th>recall_weighted</th>\n",
       "      <th>f1_weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>conceptnet</td>\n",
       "      <td>IsotonicRegression</td>\n",
       "      <td>neural_course</td>\n",
       "      <td>validation_df</td>\n",
       "      <td>42</td>\n",
       "      <td>2023-05-20 15:57:59</td>\n",
       "      <td>0.638813</td>\n",
       "      <td>0.467774</td>\n",
       "      <td>...</td>\n",
       "      <td>0.353846</td>\n",
       "      <td>0.117949</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.174242</td>\n",
       "      <td>0.353846</td>\n",
       "      <td>0.353846</td>\n",
       "      <td>0.353846</td>\n",
       "      <td>0.125207</td>\n",
       "      <td>0.353846</td>\n",
       "      <td>0.184965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>conceptnet</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>neural_course</td>\n",
       "      <td>validation_df</td>\n",
       "      <td>42</td>\n",
       "      <td>2023-05-20 15:57:59</td>\n",
       "      <td>0.698580</td>\n",
       "      <td>-0.502717</td>\n",
       "      <td>...</td>\n",
       "      <td>0.353846</td>\n",
       "      <td>0.117949</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.174242</td>\n",
       "      <td>0.353846</td>\n",
       "      <td>0.353846</td>\n",
       "      <td>0.353846</td>\n",
       "      <td>0.125207</td>\n",
       "      <td>0.353846</td>\n",
       "      <td>0.184965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id  embedding_seperated embedding_model_name classification_model_name  \\\n",
       "2       2                 True           conceptnet        IsotonicRegression   \n",
       "5       5                 True           conceptnet          LinearRegression   \n",
       "\n",
       "    dataset_name  dataset_split  seed_data_split           time_stamp  \\\n",
       "2  neural_course  validation_df               42  2023-05-20 15:57:59   \n",
       "5  neural_course  validation_df               42  2023-05-20 15:57:59   \n",
       "\n",
       "       rmse  pears_correlation  ...  accuracy  precision_macro  recall_macro  \\\n",
       "2  0.638813           0.467774  ...  0.353846         0.117949      0.333333   \n",
       "5  0.698580          -0.502717  ...  0.353846         0.117949      0.333333   \n",
       "\n",
       "   f1_macro  precision_micro  recall_micro  f1_micro  precision_weighted  \\\n",
       "2  0.174242         0.353846      0.353846  0.353846            0.125207   \n",
       "5  0.174242         0.353846      0.353846  0.353846            0.125207   \n",
       "\n",
       "   recall_weighted  f1_weighted  \n",
       "2         0.353846     0.184965  \n",
       "5         0.353846     0.184965  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wh/qyl6p0kj7_s9q0djmp51j2jw0000gn/T/ipykernel_1734/3116340303.py:4: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  performance_embed_avg = performance.groupby(grouped_column).mean().reset_index()\n",
      "/var/folders/wh/qyl6p0kj7_s9q0djmp51j2jw0000gn/T/ipykernel_1734/3116340303.py:11: FutureWarning: The default value of numeric_only in DataFrameGroupBy.std is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  performance_embed_std = performance.groupby(grouped_column).std().reset_index()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "grouped_column = \"embedding_model_name\"\n",
    "\n",
    "# embedding_model_name classification_model_name\n",
    "performance_embed_avg = performance.groupby(grouped_column).mean().reset_index()\n",
    "performance_embed_avg_classification = performance_embed_avg.loc[:, [grouped_column, \"accuracy\", \"precision_weighted\", \"recall_weighted\", \"f1_weighted\"]]\n",
    "performance_embed_avg_classification = performance_embed_avg_classification.round(2)\n",
    "\n",
    "performance_embed_avg_cont = performance_embed_avg.loc[:, [grouped_column, \"rmse\", \"pears_correlation\", \"p_value\"]]\n",
    "performance_embed_avg_cont = performance_embed_avg_cont.round(2)\n",
    "\n",
    "performance_embed_std = performance.groupby(grouped_column).std().reset_index()\n",
    "performance_embed_std_classification = performance_embed_std.loc[:, [grouped_column, \"accuracy\", \"precision_weighted\", \"recall_weighted\", \"f1_weighted\"]]\n",
    "performance_embed_std_classification = performance_embed_std_classification.round(2)\n",
    "\n",
    "performance_embed_std_cont = performance_embed_std.loc[:, [grouped_column, \"rmse\", \"pears_correlation\", \"p_value\"]]\n",
    "performance_embed_std_cont = performance_embed_std_cont.round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "embedding\\_model\\_name &  accuracy &  precision\\_weighted &  recall\\_weighted &  f1\\_weighted \\\\\n",
      "\\midrule\n",
      "          conceptnet &      0.46 &                0.48 &             0.46 &         0.41 \\\\\n",
      "            fasttext &      0.44 &                0.47 &             0.44 &         0.37 \\\\\n",
      "               glove &      0.45 &                0.46 &             0.45 &         0.38 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wh/qyl6p0kj7_s9q0djmp51j2jw0000gn/T/ipykernel_1734/1166460973.py:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(performance_embed_avg_classification.to_latex(index=False))\n"
     ]
    }
   ],
   "source": [
    "print(performance_embed_avg_classification.to_latex(index=False))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "embedding\\_model\\_name &  rmse &  pears\\_correlation &  p\\_value \\\\\n",
      "\\midrule\n",
      "          conceptnet &  0.76 &               0.31 &     0.06 \\\\\n",
      "            fasttext &  0.78 &               0.26 &     0.06 \\\\\n",
      "               glove &  0.78 &               0.27 &     0.09 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wh/qyl6p0kj7_s9q0djmp51j2jw0000gn/T/ipykernel_1734/3261326330.py:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(performance_embed_avg_cont.to_latex(index=False))\n"
     ]
    }
   ],
   "source": [
    "print(performance_embed_avg_cont.to_latex(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "embedding\\_model\\_name &  accuracy &  precision\\_weighted &  recall\\_weighted &  f1\\_weighted \\\\\n",
      "\\midrule\n",
      "          conceptnet &      0.15 &                0.19 &             0.15 &         0.18 \\\\\n",
      "            fasttext &      0.16 &                0.19 &             0.16 &         0.19 \\\\\n",
      "               glove &      0.15 &                0.19 &             0.15 &         0.19 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wh/qyl6p0kj7_s9q0djmp51j2jw0000gn/T/ipykernel_1734/1155407788.py:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(performance_embed_std_classification.to_latex(index=False))\n"
     ]
    }
   ],
   "source": [
    "print(performance_embed_std_classification.to_latex(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "embedding\\_model\\_name &  rmse &  pears\\_correlation &  p\\_value \\\\\n",
      "\\midrule\n",
      "          conceptnet &  0.20 &               0.20 &     0.21 \\\\\n",
      "            fasttext &  0.21 &               0.16 &     0.19 \\\\\n",
      "               glove &  0.21 &               0.18 &     0.28 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wh/qyl6p0kj7_s9q0djmp51j2jw0000gn/T/ipykernel_1734/594965855.py:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(performance_embed_std_cont.to_latex(index=False))\n"
     ]
    }
   ],
   "source": [
    "print(performance_embed_std_cont.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f24baac98ea445c1a3207eab7abef97e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff03a1c1d7444b09f070f97c1dee430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e77738242f4f8ba38a12a237ba39fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29c65ba87ac546d5b2777b6a10a96873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0100, -0.4312, -0.5749,  ..., -1.0529, -0.0838,  0.1126],\n",
      "         [-0.4931, -0.1674, -0.3542,  ..., -0.6042,  0.2402,  0.3796],\n",
      "         [-0.1265, -0.0057, -0.3444,  ..., -0.0752,  0.5391,  0.1031],\n",
      "         ...,\n",
      "         [-0.1106, -0.5507, -0.4991,  ...,  0.1243, -0.1096,  0.4351],\n",
      "         [-0.2631, -0.8986, -0.3745,  ..., -0.3932, -0.4494, -0.6258],\n",
      "         [-0.3255, -0.8167, -0.4563,  ..., -0.4466,  0.2009,  0.3663]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# Load pre-trained model tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "\n",
    "# Load pre-trained model\n",
    "model = BertModel.from_pretrained('bert-large-uncased')\n",
    "\n",
    "# Sentence for which we want to create embeddings\n",
    "sentence = \"This is a sample sentence.\"\n",
    "\n",
    "# Tokenize the sentence and convert to input IDs.\n",
    "inputs = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "# Run the sentence through the model to get the embeddings.\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# We get a tuple as output. The first element of the tuple is the embeddings.\n",
    "embeddings = outputs[0]\n",
    "\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2023, 2003, 1037, 7099, 6251, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1400,  0.2273, -0.2721,  ..., -0.0171,  0.2993,  0.8581],\n",
      "         [-0.6647,  0.1726, -0.0928,  ..., -0.1377,  0.6326,  0.6879],\n",
      "         [-0.6669, -0.1006, -0.1085,  ..., -0.0977, -0.1692,  1.0540],\n",
      "         ...,\n",
      "         [ 0.7031, -0.7658,  0.5756,  ...,  0.3718,  0.5955, -0.1600],\n",
      "         [ 0.4393, -0.7209,  0.1950,  ...,  0.7028,  0.5261, -0.3388],\n",
      "         [ 1.1341,  0.0862, -0.5087,  ...,  0.1970, -0.8134, -0.1709]]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# Load pre-trained model tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pre-trained model\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# A very long sentence\n",
    "sentence = \"This is a very long sentence...\" # Assume this is a very long sentence\n",
    "\n",
    "# Tokenize the sentence and get the tokens\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "# Define the maximum chunk size (max BERT length - 2 for [CLS] and [SEP])\n",
    "max_chunk_size = 510\n",
    "\n",
    "# Split the tokens into chunks\n",
    "chunks = [tokens[i:i + max_chunk_size] for i in range(0, len(tokens), max_chunk_size)]\n",
    "\n",
    "# For each chunk, add special tokens, convert to input IDs, feed to BERT, and get embeddings\n",
    "for chunk in chunks:\n",
    "    # Add special tokens\n",
    "    chunk = ['[CLS]'] + chunk + ['[SEP]']\n",
    "    # Convert to input IDs\n",
    "    inputs = tokenizer.convert_tokens_to_ids(chunk)\n",
    "    inputs = torch.tensor([inputs])  # Add batch dimension\n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "    # The first element of outputs is the embeddings\n",
    "    embeddings = outputs[0]\n",
    "    print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
